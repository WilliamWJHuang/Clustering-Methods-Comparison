{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Methods Comparison\n",
    "\n",
    "##### Descriptions of Clustering Methods\n",
    "- **K-means clustering technique** <br>K-means clustering finds the k numbers of centroids. It measures the euclidean distances between observations and centroids and assigns them to the corresponding groups based on the shortest euclidean distance. This process will reiterte several times till the centroids are fixed. The key difference of Kmeans clustering is it could find k clusters.\n",
    "- **Agglomerative clustering** <br>Agglomerative clustering defines each observation as a cluster in the beginning. Then clusters are merged together into new different clusters based on the similarity based on the minimizing linkage criteria, which measures the similarity. What differentiates this clustering technique is it doesn't require a specified k and with a tree model.\n",
    "- **Gaussian mixture model** <br>Gaussian mixture model is a category of probabilistic model which states that all generated data points are derived from a mixture of a finite Gaussian distributions that has no known parameters. This algorithm could use its ability to identify non-flat geometry or uneven cluster sizes.\n",
    "- **DBSCAN clustering** <br>DBSCAN clustering separates the data according to its density. What makes this algorithm different is that it's capable of clustering non-flat geometry and uneven cluster sizes. However, when handling data with high dimensionality, the distance threshold would be hard to estimate. \n",
    "- **Spectral clustering technique** <br>Spectral clustering technique starts the following steps :(1)Create a similarity graph between our N objects to cluster.(2)Compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object.(3)Run k-means on these features to separate objects into k classes. However, spectral clustering is computationally expensive unless the graph is sparse and the similarity matrix can be efficiently constructed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GaussianMixture' object has no attribute 'fit_predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-355461672283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#GaussianMixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0maggregation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAggregation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mcompound_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0md31_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GaussianMixture' object has no attribute 'fit_predict'"
     ]
    }
   ],
   "source": [
    "Aggregation = pd.read_csv(\"Aggregation.txt\", sep=\"\\t\", header=None).values\n",
    "Compound = pd.read_csv(\"Compound.txt\", sep=\"\\t\", header=None).values\n",
    "d31 = pd.read_csv(\"D31.txt\", sep=\"\\t\", header=None).values\n",
    "jain = pd.read_csv(\"jain.txt\", sep=\"\\t\", header=None).values\n",
    "\n",
    "#model = [K-means,AgglomerativeClustering,GaussianMixture,DBSCAN,SpectralClustering]\n",
    "aggregation_model=list()\n",
    "compound_model=list()\n",
    "d31_model=list()\n",
    "jain_model=list()\n",
    "\n",
    "#K-means\n",
    "aggregation_model.append(KMeans(7).fit_predict(Aggregation))\n",
    "compound_model.append(KMeans(5).fit_predict(Compound))\n",
    "d31_model.append(KMeans(31).fit_predict(d31))\n",
    "jain_model.append(KMeans(2).fit_predict(jain))\n",
    "\n",
    "#AgglomerativeClustering\n",
    "aggregation_model.append(AgglomerativeClustering(7).fit_predict(Aggregation))\n",
    "compound_model.append(AgglomerativeClustering(6).fit_predict(Compound))\n",
    "d31_model.append(AgglomerativeClustering(31).fit_predict(d31))\n",
    "jain_model.append(AgglomerativeClustering(2, linkage=\"complete\").fit_predict(jain))\n",
    "\n",
    "\n",
    "#GaussianMixture\n",
    "aggregation_model.append(GaussianMixture(n_components=7, covariance_type='full').fit_predict(Aggregation))\n",
    "compound_model.append(GaussianMixture(n_components=6, covariance_type='full').fit_predict(Compound))\n",
    "d31_model.append(GaussianMixture(n_components=31, covariance_type='full').fit_predict(d31))\n",
    "jain_model.append(GaussianMixture(n_components=2, covariance_type='full').fit_predict(jain))\n",
    "\n",
    "#DBSCAN\n",
    "aggregation_model.append(DBSCAN(eps=2, min_samples=2).fit_predict(Aggregation))\n",
    "compound_model.append(DBSCAN(eps=1.6, min_samples=2).fit_predict(Compound))\n",
    "d31_model.append(DBSCAN(eps=1.6, min_samples=2).fit_predict(d31))\n",
    "jain_model.append(DBSCAN(eps=2.7, min_samples=2).fit_predict(jain))\n",
    "\n",
    "#SpectralClustering\n",
    "aggregation_model.append(SpectralClustering(n_clusters=7).fit_predict(Aggregation))\n",
    "compound_model.append(SpectralClustering(n_clusters=6).fit_predict(Compound))\n",
    "d31_model.append(SpectralClustering(n_clusters=31).fit_predict(d31))\n",
    "jain_model.append(SpectralClustering(n_clusters=2).fit_predict(jain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = [\"k-means\", \"Agglomerative\", \"Gaussian mixture\", \n",
    "              \"DBSCAN\", \"Spectral\"]\n",
    "\n",
    "i = 1\n",
    "plt.figure(figsize=(17, 10))\n",
    "for agre_clusters, comp_clusters, d31_clusters, jain_clusters \\\n",
    "    in zip(aggregation_model, compound_model, d31_model, jain_model):\n",
    "    plt.subplot(4, 5, i)\n",
    "    plt.scatter(Aggregation[:, 0], Aggregation[:, 1], c=agre_clusters, cmap=\"Accent\")\n",
    "    plt.title(\"{} for 'Aggregation data'\".format(model_name[(i % 5) - 1]))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.subplot(4, 5, 5+i)\n",
    "    plt.scatter(Compound[:, 0], Compound[:, 1], c=comp_clusters, cmap=\"Accent\")\n",
    "    plt.title(\"{} for 'Compound data'\".format(model_name[(i % 5) - 1]))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.subplot(4, 5, 10+i)\n",
    "    plt.scatter(d31[:, 0], d31[:, 1], c=d31_clusters, cmap=\"Accent\")\n",
    "    plt.title(\"{} for 'd31 data'\".format(model_name[(i % 5) - 1]))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.subplot(4, 5, 15+i)\n",
    "    plt.scatter(jain[:, 0], jain[:, 1], c=jain_clusters, cmap=\"Accent\")\n",
    "    plt.title(\"{} for 'jain data'\".format(model_name[(i % 5) - 1]))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings from the datasets above \n",
    "- **Aggregation.txt**: spectral clustering, agglomerative clustering, and gaussian mixture model capture the dataset fairly well. Although Gaussian distribution struggled with the bottom middle circle region, it's still capable of capturing the other clusters. \n",
    "\n",
    "- **Compound.txt**: DBSCAN clustering works well on this dataset (eg lower left clusters). Gaussian mixture model and kmeans work well for the upper left clusters, since these two clusters are gaussians-centered for the mean, with identical covarience matrices. Agglomerative clustering didn't work well in this dataset as many clusters does not have adequate distance to differentiate them.\n",
    "\n",
    "- **d31.txt**: All the clustering techniques perform fairly well. But it's worth noticing that it takes a long time to run spectral clustering.\n",
    "\n",
    "- **jain.txt**: Spectral clustering works well for this dataset since clusters have strong connectedness, which aligns with the assumption of spectral clustering. On the other hand, Kmeans clustering doesn't work well, because kmeans does not work well on non-centroids dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
